{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning\n",
    "#### First step in sampling from WDS\n",
    "\n",
    "This notebook only shows that we can run one step of active learning on WDS output.\n",
    "More work would need to be done to complete the active learning cycle.\n",
    "\n",
    "This currently uses *uncertainty sampling* to select passages to annotate, but this could be revisited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import json\n",
    "import pandas as pd\n",
    "from ibm_watson import DiscoveryV1\n",
    "from ibm_cloud_sdk_core.authenticators import BasicAuthenticator\n",
    "\n",
    "from toal.stores import BasicStore\n",
    "from toal.stores.loaders import load_from_watson_discover, load_w_annotation_units_from_watson_discover\n",
    "from toal.learners import MulticlassWatsonDiscoveryLearner\n",
    "from toal.samplers import MulticlassUncertaintySampler, MulticlassDensitySampler, MulticlassClusterSampler, MulticlassComplexSampler\n",
    "\n",
    "# just to make results easier to display in notebook\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.width=None\n",
    "pd.options.display.max_colwidth = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"add here\"\n",
    "password = \"add here\"\n",
    "url= \"add here\"\n",
    "version = \"add here\"\n",
    "collection_id = \"add here\"\n",
    "environment_id = \"add here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to WDS\n",
    "authenticator = BasicAuthenticator(username, password)\n",
    "wds = DiscoveryV1(\n",
    "    version=version,\n",
    "    authenticator=authenticator)\n",
    "\n",
    "wds.set_service_url(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a trained (WKS) model and we can query a (WDS) collection to get (WKS) predicitons on that unlabeled collection data.\n",
    "\n",
    "You can filter the respose by adding entities or relations to the query, or use `None` to use active learning over everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) filter by entity type\n",
    "# toi = \"SELL_ACQ\"  # 'type' of interest\n",
    "# query = \"enriched_text.entities.type:\" + toi\n",
    "# 2) filter by relation type\n",
    "toi = \"transacted\"  # type of interest  is_acquiring  transacted is_promoted\n",
    "query = \"enriched_text.relations.type:\" + toi\n",
    "# 3) use None to avoid filtering by entity or relation type\n",
    "# toi = None\n",
    "# query = None\n",
    "\n",
    "# query(self, environment_id, collection_id, filter=None, query=None, natural_language_query=None, passages=None, aggregation=None, \n",
    "#       count=None, return_=None, offset=None, sort=None, highlight=None, passages_fields=None, passages_count=None, \n",
    "#       passages_characters=None, deduplicate=None, deduplicate_field=None, similar=None, similar_document_ids=None, \n",
    "#       similar_fields=None, bias=None, spelling_suggestions=None, x_watson_logging_opt_out=None, **kwargs)\n",
    "q_response = wds.query(environment_id, collection_id, query=query, count=1000).get_result()\n",
    "\n",
    "print(\"Query matched: %s\" % q_response['matching_results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to use active learning to find the most promising data to annotate to improve the machine learning model.  We start with *uncertainty* sampling with *least confidence*.  This is a simple but effective baseline that looks for the instances the model is least confident to annotate to improve the model.\n",
    "\n",
    "Optionally we can apply clustering with the active learning that will be sure the pick a greater variety of uncertain examples.  However, this may also sample from some high confidence clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# annotation batch size\n",
    "batch_size = min(50, len(q_response['results']))\n",
    "enrichment = 'relations'  # entities or relations\n",
    "\n",
    "# use active learning toolkit\n",
    "store = BasicStore()\n",
    "store.append_data( *load_from_watson_discover(q_response, enrichment=enrichment, filter_type=toi))\n",
    "#store.append_data( *load_w_annotation_units_from_watson_discover(q_response, enrichment=enrichment, filter_type=toi))\n",
    "\n",
    "learner = MulticlassWatsonDiscoveryLearner(q_response, enrichment=enrichment, filter_type=toi)\n",
    "\n",
    "# basic (but still effective) sampler\n",
    "sampler = MulticlassUncertaintySampler(learner, strategy='lc')\n",
    "# sampler with clustering\n",
    "# sampler = MulticlassClusterSampler(learner)\n",
    "sampled_df = sampler.choose_instances(store, batch_size=20)\n",
    "sampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samplers above treat each machine learning instance (e.g., entity or relation) separately and try to choose the best for annotation.  However, annotation is done over sentences or passages.  The active learning sampling below tries to choose the best sentences/passages (comprising perhaps multiple entites/relations) for annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate predictions across a sentence\n",
    "store = BasicStore()\n",
    "store.append_data( *load_w_annotation_units_from_watson_discover(q_response, enrichment=enrichment, filter_type=toi))\n",
    "sampler = MulticlassComplexSampler(learner)\n",
    "sampled_df = sampler.choose_instances(store, batch_size=batch_size, selection='least_worst') # sum mean best least_worst\n",
    "sampled_df['text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
